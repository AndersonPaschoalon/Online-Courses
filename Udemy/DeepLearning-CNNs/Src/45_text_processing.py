# -*- coding: utf-8 -*-
"""45_Text_processing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_xGU-PWZqore3JkO65RkVbeGTXW8NslK
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences


print(tf.__version__)

# just a simple test
sentences = [
    "I like eggs and ham.",
    "I love chocolate and bunnies.",
    "I hate onions."
]

MAX_VOCAB_SIZE = 20000
tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)
tokenizer.fit_on_texts(sentences)
sequences = tokenizer.texts_to_sequences(sentences)
print("sequences:", sequences)

# how to get the word to index mapping?
print("tokenizer.word_index:", tokenizer.word_index)
tokenizer.word_index

# ude defaults
data = pad_sequences(sequences)
print("data:", data)

MAX_SEQUENCE_LENGTH = 5
data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
print("data:", data)

data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')
print("data:", data)

# too much padding
data = pad_sequences(sequences, maxlen=6)
print("data:", data)

# truncation
data = pad_sequences(sequences, maxlen=4)
print("data:", data)

data = pad_sequences(sequences, maxlen=4, truncating='post')
print("data:", data)