# -*- coding: utf-8 -*-
"""47_text_classification_cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13IkaAGfkElS7QM00wdSBZfVDQ_0bRqJe
"""
import os.path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding
from tensorflow.keras.models import Model

out_dir = "47"
params = {"epochs": 15}
span_file = os.path.join(out_dir, "spam.csv")
print(tf.__version__)

print(f"Spam file is {span_file}")
# df = pd.read_csv('spam.csv', encoding='ISO-8859-1')
df = pd.read_csv(span_file, encoding='ISO-8859-1')
df.head()

#drop unnecessary columns
df = df.drop(["Unnamed: 2", "Unnamed: 3", "Unnamed: 4"], axis=1)
df.head()

df.columns = ['labels', 'data']
df.head()

df['b_labels'] = df['labels'].map({'ham':0, 'spam':1})
Y = df['b_labels'].values

# split up the data
df_train, df_test, Ytrain, Ytest = train_test_split(df['data'], Y, test_size=0.33)

# convert sentences to sequences
MAX_VOCAB_SIZE = 20000
tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)
tokenizer.fit_on_texts(df_train)
sequences_train = tokenizer.texts_to_sequences(df_train)
sequences_test = tokenizer.texts_to_sequences(df_test)

# get word -> integer mapping
word2idx = tokenizer.word_index
V = len(word2idx)
print("Found %s unique tokens." %V)

# pad sequences so that we get a N x T matrix
data_train = pad_sequences(sequences_train)
print("Shape of data train tensor:", data_train.shape)

# get sequence lenght
T = data_train.shape[1]
print(T)

data_test = pad_sequences(sequences_test, maxlen=T)
print("Shape of data test tensor:", data_test.shape)

# crete the model

D = 20
i = Input(shape=(T,))
x = Embedding(V + 1, D)(i)

x = Conv1D(32, 3, activation='relu')(x)
x = MaxPooling1D(3)(x)

x = Conv1D(64, 3, activation='relu')(x)
x = MaxPooling1D(3)(x)

x = Conv1D(128, 3, activation='relu')(x)
x = GlobalMaxPooling1D()(x)
x = Dense(1, activation='sigmoid')(x)

model = Model(i, x)


model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.summary()

print("Training model....")
r = model.fit(data_train, Ytrain, epochs=params["epochs"], validation_data=(data_test, Ytest))

plt.clf()
# plot the loss per iteration
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()
# plt.show()
print(f"Saving plot {out_dir}")
plt.savefig(os.path.join(out_dir, "val_loss"))

plt.clf()
# plot the accuracy per iteration
plt.plot(r.history['accuracy'], label='accuracy')
plt.plot(r.history['val_accuracy'], label='val_accuracy')
plt.legend()
# plt.show()
print(f"Saving plot {out_dir}")
plt.savefig(os.path.join(out_dir, "val_accuracy"))

