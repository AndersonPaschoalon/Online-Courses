# -*- coding: utf-8 -*-
"""15_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RssnmygH3LGO_TrV7o3tdKDfxtOequYH

# Linear Regression
"""

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Input, Dense, Flatten, Dropout
from tensorflow.keras.models import Model

out_dir = "15"
moore_path = f"{out_dir}/moore.csv"

# !wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv

data = pd.read_csv(moore_path, header=None).to_numpy()
# data = pd.read_csv('15/moore.csv', head=None)

X = data[:, 0].reshape(-1, 1) # make an Nx D array
Y = data[:, 1]
print(data.shape)
print(X.shape)
print(Y.shape)
plt.clf()
plt.scatter(X, Y)
plt.savefig(f"{out_dir}/scater_plt")

Yl = np.log(Y)
plt.scatter(X, Yl)

Xnorm = X - X.mean()

#i = Input(shape=(1,))
#x = Dense(1)(i)
#model = Model(i, x)
model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(1,)),
    tf.keras.layers.Dense(1)
])

model.compile(
   optimizer=tf.keras.optimizers.SGD(0.001, 0.9),
   loss='mse'
)

# scheduler for the learning rate: changes the learning rate during the optimization
def schedule(epoch, lr):
  if epoch >= 50:
    return 0.0001
  elif epoch >= 100:
    return 0.00001
  elif epoch >= 150:
    return 0.000001
  return 0.001

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)

r = model.fit(Xnorm, Yl, epochs=200, callbacks=[scheduler])
#r = model.fit(Xnorm, Yl, epochs=200)

# plot the loss
plt.clf()
plt.plot(r.history['loss'], label='loss')
plt.legend()
plt.savefig(f"{out_dir}/loss")

a = model.layers[0].get_weights()[0][0,0]
b = model.layers[0].get_weights()[1][0]

print(f"a:{a}, b:{b}");