{"cells":[{"cell_type":"markdown","metadata":{"id":"SWuqssKwZY-e"},"source":["https://github.com/alura-cursos/1563-treinando-pytorch/blob/aula-04/Carregamento%20de%20Dados.ipynb\n"]},{"cell_type":"markdown","metadata":{"id":"ddBbXTLUGYHQ"},"source":["Bilbioteca de dados do Pytorch:\n","torchtext https://pytorch.org/text/stable/index.html\n","torchvision https://pytorch.org/vision/stable/index.html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":294,"status":"ok","timestamp":1653685102270,"user":{"displayName":"Anderson P","userId":"07518979327659766490"},"user_tz":180},"id":"7wRouxUDCZP2","outputId":"53cfd322-3c07-4fde-c910-51d72910049f"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n","Amostras de treino :60000\n","Amostras de teste: 10000\n"]}],"source":["# Carregamento de dados\n","\n","import torch\n","from torch import nn, optim\n","\n","from torchvision import datasets\n","from torchvision import transforms \n","from torchvision import transforms\n","\n","\n","\n","args = {\n","    'batch_size': 5,\n","    'num_workers': 4,\n","    'num_classes': 10,\n","    'lr': 1e-4,\n","    'weight_decay': 5e-4,\n","    'num_epochs': 30\n","}\n","\n","\n","if torch.cuda.is_available():\n","  args['device'] = torch.device('cuda')\n","else:\n","  args['device'] = torch.device('cpu')\n","\n","print(args['device'])\n","\n","train_set = datasets.MNIST('./', \n","                      train=True, \n","                      transform=transforms.ToTensor(), \n","                      target_transform=None, \n","                      download=True)\n","\n","test_set = datasets.MNIST('./', \n","                      train=False, \n","                      transform=transforms.ToTensor(), \n","                      target_transform=None, \n","                      download=False)\n","\n","\n","print(\"Amostras de treino :\" + str(len(train_set)) + \"\\nAmostras de teste: \" + str(len(test_set)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":845},"executionInfo":{"elapsed":886,"status":"ok","timestamp":1653685106202,"user":{"displayName":"Anderson P","userId":"07518979327659766490"},"user_tz":180},"id":"WQqN0517TbZ1","outputId":"61c1aa7d-00d8-42fc-e7f2-c0ddd1312db4"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'torchvision.datasets.mnist.MNIST'>\n","<class 'tuple'>\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQ/ElEQVR4nO3de7BdZX3G8e+T9JBALgzhEsPNCCRipJLAEURRUAoTYBSYKpdpMaV04gxCQbBKkSmMnWmpgyIqSCPEBNBQplzbUiSkqRRB5ACBAOFmTCQhJJIghFtIcn79Y684x3DWu0/2/Zz3+czsOfus337X+rHJc9bee621X0UEZjb0DWt3A2bWGg67WSYcdrNMOOxmmXDYzTLhsJtlwmG3qiRdKunGdvdh9XHYBylJyyS9LekNSS9LmiNp9DaM/bNm91iy7YmSFkp6S9Iz7eojRw774PbZiBgNTAWmAX/f5n4GYh7wGLAz8A3g3yXt2t6W8uCwDwER8TLwMyqhB0DS5yQ9Jen3kv5X0oeK5TcAewP/Ubwq+JqkIyWt6LvO1N6/bN3VSJoMHARcEhFvR8QtwGLgz2v577Zt47APAZL2BI4FXih+n0xlD3oesCtwF5VwbxcRpwO/pXhVEBHf2sZtla67qF8t6eqS4R8GlkbE+j7LHi+WW5M57IPb7ZLWAy8Ca4BLiuWnAP8VEfMjYiNwObA98PEGbDO57og4KyLOKhk7Gnhtq2WvAWMa0JdV4bAPbidGxBjgSGB/YJdi+e7A8i0PioheKn8Q9mjANutZ9xvA2K2WjQXW9/NYazCHfQiIiJ8Dc6jsZQFeAt6/pS5JwF7Ayi1DtlrFm8AOfR4/nMpL9P5UW3fKU8A+kvruyQ8slluTOexDx3eBoyUdCNwMHC/pKEldwAXABuCB4rGrgX36jH0OGCnp+OLxFwMjSrZTbd2lIuI5YBFwiaSRkk4CPgLcso3/rVYDh32IiIjfAdcD/xARzwJ/CXwfeAX4LJUP5N4tHv7PwMXFp+lfjYjXgLOAa6nsod8EVmy9jWI7yXVLukbSNYlWTwW6gVeBy4DPF71bk8lfXmGWB+/ZzTLhsJtlwmE3y4TDbpaJP2nlxrbTiBjJqFZu0iwr7/Am78YG9VerK+ySpgNXAsOBayPistTjRzKKQ3VUPZs0s4SHYkFpreaX8cVZVldRuQBjCnCapCm1rs/Mmque9+yHAC9ExNLihIqbgBMa05aZNVo9Yd+DygUQW6ygn4shJM2U1COpZyMb6ticmdWj6Z/GR8SsiOiOiO6u0tOtzazZ6gn7SipXO22xJwO78snM2qCesD8MTJL0geJbSk4F7mxMW2bWaDUfeouITZLOpvLdZ8OB2RHh65LNOlRdx9kj4i4q30FmZh3Op8uaZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmWjplsw09mz5zcLK+6qzyKb8eP2xucuyBD85I1ne/artkffjCR5P13HjPbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwsfZLan3iGnJ+vdm/yBZ36+r/J9Yb5VtP3bYj5P1Z7s3J+t/N/FjVbaQl7rCLmkZsB7YDGyKiO5GNGVmjdeIPfunI+KVBqzHzJrI79nNMlFv2AO4R9Ijkmb29wBJMyX1SOrZSPl50mbWXPW+jD88IlZK2g2YL+mZiLiv7wMiYhYwC2CsxkWd2zOzGtW1Z4+IlcXPNcBtwCGNaMrMGq/msEsaJWnMlvvAMcCTjWrMzBqrnpfx44HbJG1Zz08j4u6GdGUts/GY9NHSr119Q7I+uSt9TXlv4mj60o0bk2Nf6x2RrE9Ll9lw7EdLa9svXJwc2/vOO+mVD0I1hz0ilgIHNrAXM2siH3ozy4TDbpYJh90sEw67WSYcdrNM+BLXIWD42LGltTc/tX9y7Feu+Gmy/unt36iy9dr3F3Ne/XiyvuDqw5L1X1z6vWR9/rXXlNam3Hh2cuw+X38wWR+MvGc3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLh4+xDwIrr9yitPfzRq1rYybb55m4PJ+t3j04fhz9j2THJ+tyJ95bWxk5Zmxw7FHnPbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwsfZB4FNnzk4WZ83tXza5GGkv+q5mjOWH5Ws99z7oWR98ZnlvS18e2Ry7G49byfrL7yavla/658WltaGKTl0SPKe3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhCKiZRsbq3FxqNLHbXPUe8S0ZP27c69O1vfrqv10ic89c1KyPvzzbybr647/YLK+9oDyA9qTr3oxOXbTiyuS9Wr+c+UjpbVVm9PH8P96xt8m68MXPlpTT832UCzg9VjX75Nedc8uabakNZKe7LNsnKT5kp4vfu7UyIbNrPEG8jJ+DjB9q2UXAgsiYhKwoPjdzDpY1bBHxH3Auq0WnwDMLe7PBU5scF9m1mC1vtkbHxGrivsvA+PLHihpJjATYCQ71Lg5M6tX3Z/GR+UTvtJP+SJiVkR0R0R3FyPq3ZyZ1ajWsK+WNAGg+LmmcS2ZWTPUGvY7gRnF/RnAHY1px8yapep7dknzgCOBXSStAC4BLgNulnQmsBw4uZlNDnY6+MPJ+ivnp4/5Tu5KX5P+yIby2v+8MSU5du1NeyXrO7+anqd8xxt/ma4napuSI5tr/PD0W8q1572VrO9Wfql8x6oa9og4raTks2PMBhGfLmuWCYfdLBMOu1kmHHazTDjsZpnwV0k3wLAd0qcBb/rW68n6L/e/NVn/zaZ3k/XzL7qgtLbT//02OXa3UenzoTYnq0PXIROWJ+vLWtNGQ3nPbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwsfZG+DtI9KXsP5s//RXQVfzN+d+JVkfc3v5ZabtvIzUOov37GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJnycvQE+8o+LkvVhVf6mnrE8/UW929/+q23uyaBLw0trG6vMVD5crZvKvFW8ZzfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuHj7AP0+9MPK61dPP7y5Nheqky5fE96WuW9eSBZt/5tjPJvve+lNzn27iXp/yeTeLSmntqp6p5d0mxJayQ92WfZpZJWSlpU3I5rbptmVq+BvIyfA0zvZ/kVETG1uN3V2LbMrNGqhj0i7gPWtaAXM2uiej6gO1vSE8XL/J3KHiRppqQeST0b2VDH5sysHrWG/YfAvsBUYBXw7bIHRsSsiOiOiO4uRtS4OTOrV01hj4jVEbE5InqBHwGHNLYtM2u0msIuaUKfX08Cnix7rJl1hqrH2SXNA44EdpG0ArgEOFLSVCCoTFX9pSb22BE2bV9e23FY+jj6g++k377sc/1L6W0nq0NXtXnvn7n8gCpreKS08hdLj02O3P/c3yTrg3He+qphj4jT+ll8XRN6MbMm8umyZplw2M0y4bCbZcJhN8uEw26WCV/i2gJrN49O1jctXdaaRjpMtUNrz172p8n6Myf8IFn/77d2LK29dNV+ybFjXi2fBnuw8p7dLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEj7O3wFd/8YVkfXLiUszBrveIaaW1Nee/nRy7pDt9HP2oxack66OmLy2tjWHoHUevxnt2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTPs4+UCovDavyN/PKw+cl61cxuZaOOsLyb5ZPZQ1wyxe/U1qb3JX+Cu6DfjUjWd/9pKeTdftj3rObZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZpkYyJTNewHXA+OpTNE8KyKulDQO+DdgIpVpm0+OiFeb12qbRXmpl97k0CO2X5usnzfn4GR93x+n19/18vrS2uojdk2OHXfKimT9nL0XJOvH7pC+Fv/ON8eX1r64eHpy7C7/OipZt20zkD37JuCCiJgCfAz4sqQpwIXAgoiYBCwofjezDlU17BGxKiIeLe6vB5YAewAnAHOLh80FTmxWk2ZWv216zy5pIjANeAgYHxGritLLVF7mm1mHGnDYJY0GbgHOi4jX+9YiIih5VytppqQeST0b2VBXs2ZWuwGFXVIXlaD/JCJuLRavljShqE8A1vQ3NiJmRUR3RHR3MaIRPZtZDaqGXZKA64AlEdH3EqY7gS2XJc0A7mh8e2bWKAO5xPUTwOnAYkmLimUXAZcBN0s6E1gOnNycFge/kUo/zUuOviZZv/+TI5P15ze8r7R2xo7LkmPrde5Ln0zW735gamlt0rn5fZ1zO1UNe0TcT/nV3Ec1th0zaxafQWeWCYfdLBMOu1kmHHazTDjsZplw2M0yocqZrq0xVuPiUA3Oo3XDJ+9bWps8b3ly7L+878G6tl3tq6qrXWKb8tiG9LpP+/nMZH3yGUN3uunB6KFYwOuxrt9D5d6zm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZ8JTNA7T5uV+X1p7/wsTk2CnnnJOsP33y92tpaUD2v+usZP2DV7+VrE9+zMfRhwrv2c0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTPh6drMhxNezm5nDbpYLh90sEw67WSYcdrNMOOxmmXDYzTJRNeyS9pK0UNLTkp6SdG6x/FJJKyUtKm7HNb9dM6vVQL68YhNwQUQ8KmkM8Iik+UXtioi4vHntmVmjVA17RKwCVhX310taAuzR7MbMrLG26T27pInANOChYtHZkp6QNFvSTiVjZkrqkdSzkQ11NWtmtRtw2CWNBm4BzouI14EfAvsCU6ns+b/d37iImBUR3RHR3cWIBrRsZrUYUNgldVEJ+k8i4laAiFgdEZsjohf4EXBI89o0s3oN5NN4AdcBSyLiO32WT+jzsJOAJxvfnpk1ykA+jf8EcDqwWNKiYtlFwGmSpgIBLAO+1JQOzawhBvJp/P1Af9fH3tX4dsysWXwGnVkmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8tES6dslvQ7YHmfRbsAr7SsgW3Tqb11al/g3mrVyN7eHxG79ldoadjfs3GpJyK629ZAQqf21ql9gXurVat688t4s0w47GaZaHfYZ7V5+ymd2lun9gXurVYt6a2t79nNrHXavWc3sxZx2M0y0ZawS5ou6VlJL0i6sB09lJG0TNLiYhrqnjb3MlvSGklP9lk2TtJ8Sc8XP/udY69NvXXENN6Jacbb+ty1e/rzlr9nlzQceA44GlgBPAycFhFPt7SREpKWAd0R0fYTMCR9CngDuD4iDiiWfQtYFxGXFX8od4qIr3dIb5cCb7R7Gu9itqIJfacZB04E/oo2PneJvk6mBc9bO/bshwAvRMTSiHgXuAk4oQ19dLyIuA9Yt9XiE4C5xf25VP6xtFxJbx0hIlZFxKPF/fXAlmnG2/rcJfpqiXaEfQ/gxT6/r6Cz5nsP4B5Jj0ia2e5m+jE+IlYV918GxrezmX5Unca7lbaaZrxjnrtapj+vlz+ge6/DI+Ig4Fjgy8XL1Y4UlfdgnXTsdEDTeLdKP9OM/0E7n7tapz+vVzvCvhLYq8/vexbLOkJErCx+rgFuo/Omol69ZQbd4ueaNvfzB500jXd/04zTAc9dO6c/b0fYHwYmSfqApO2AU4E729DHe0gaVXxwgqRRwDF03lTUdwIzivszgDva2Msf6ZRpvMumGafNz13bpz+PiJbfgOOofCL/a+Ab7eihpK99gMeL21Pt7g2YR+Vl3UYqn22cCewMLACeB+4FxnVQbzcAi4EnqARrQpt6O5zKS/QngEXF7bh2P3eJvlryvPl0WbNM+AM6s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwT/w+m7BvlEmICsQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","\n","\n","# classe de treino\n","print(type(train_set))\n","\n","# cada elemento é uma tupla rotulo:dado\n","print(type(train_set[0]))\n","\n","\n","for i in range(3):\n","  dado, rotulo  = train_set[i]\n","  plt.figure()\n","  plt.imshow(dado[0])\n","  plt.title('Rotulo: '  + str(rotulo))\n","  plt\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"elapsed":694,"status":"ok","timestamp":1653685111032,"user":{"displayName":"Anderson P","userId":"07518979327659766490"},"user_tz":180},"id":"px0z9mEIVs6G","outputId":"56402978-29be-462a-f9df-14fdb956dbd2"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([5, 1, 28, 28]) torch.Size([5])\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["from torch.utils.data import DataLoader\n","\n","\n","train_loader = DataLoader(train_set, \n","                          batch_size=args['batch_size'],\n","                          shuffle=True,\n","                          num_workers=args['num_workers'])\n","\n","\n","train_loader = DataLoader(test_set, \n","                          batch_size=args['batch_size'],\n","                          shuffle=True,\n","                          num_workers=args['num_workers'])\n","\n","\n","for batch in train_loader:\n","  dado, rotulo = batch\n","\n","  print(dado.size(), rotulo.size())\n","  plt.imshow(dado[0][0])\n","  plt.title('Rotulo: ' + str(rotulo[0]))\n","  break\n","\n","# batch de tamanho 20\n","# amostra de tamanho 1/28/18\n","# ou seja, uma imagem quadrada 28/28 com 1 canal de cor.\n","# torch.Size([5, 1, 28, 28])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":499},"executionInfo":{"elapsed":6258,"status":"error","timestamp":1653685122296,"user":{"displayName":"Anderson P","userId":"07518979327659766490"},"user_tz":180},"id":"O32Z7sNLRYc4","outputId":"204d2e35-6d9c-44f7-963c-2e052ffbb8d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-d15ac257e407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mdado\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotulo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mrecv_handle\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;34m'''Receive a handle over a local connection.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF_UNIX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrecvfds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mDupFd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mrecvfds\u001b[0;34m(sock, size)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mbytes_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mancdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecvmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMSG_SPACE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mancdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["class MLP(nn.Module):\n","\n","  def __init__(self, input_size, hidden_size, out_size):\n","    super(MLP, self).__init__()\n","\n","    self.features  = nn.Sequential(\n","                      nn.Linear(input_size, hidden_size),\n","                      nn.ReLU(),\n","                      nn.Linear(hidden_size, hidden_size),\n","                      nn.ReLU()\n","                    )\n","    self.out     = nn.Linear(hidden_size, out_size)\n","    self.softmax = nn.Softmax()\n","\n","  def forward(self, X):\n","    X = X.view(X.size(0), -1)\n","    feature = self.features(X)\n","    output  = self.softmax(self.out(feature))\n","\n","    return output\n","\n","\n","if torch.cuda.is_available():\n","  args['device'] = torch.device('cuda')\n","else:\n","  args['device'] = torch.device('cpu')\n","\n","print(args['device'])\n","\n","input_size = 28*28\n","hidden_size = 128\n","out_size = 10 # classes\n","torch.manual_seed(42)\n","device = args['device']\n","net = MLP(input_size, hidden_size, out_size).to(device) # cast na GPU\n","\n","criterion = nn.CrossEntropyLoss().to(args['device'])\n","# taxa de aprendizado 10^-4\n","# wight decay = 5*10^-4\n","# grid search, random search -> métodos para encontrar bons valores de parâmetros para você\n","optimizer = optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n","\n","# Fluxo de Treinamento\n","# \n","# Agora vamos aplicar o conhecimento que acabamos de aprender!\n","# \n","# Relembrando o passo a passo do fluxo de treinamento:\n","# \n","#     Iterar nas épocas\n","#     Iterar nos batches\n","#     Cast dos dados no dispositivo de hardware\n","#     Forward na rede e cálculo da loss\n","#     Cálculo do gradiente e atualização dos pesos\n","# \n","# Para acompanhar a convergência do seu modelo (e garantir que tudo foi feito certinho), ao final de cada época podemos imprimir a média e o desvio padrão das # perdas de cada iteração.\n","\n","for epoch in range(args['num_epochs']):\n","  start = time.time()\n","\n","  epoch_loss = []\n","  for batch in train_loader:\n","    \n","    dado, rotulo = batch\n","\n","    # Cast na GPU\n","    dado   = dado.to(args['device'])\n","    rotulo = rotulo.to(args['device'])\n","\n","    # Forward \n","    pred = net(dado)\n","    loss = criterion(pred, rotulo)\n","    epoch_loss.append(loss.cpu().data)\n","\n","    # Backward\n","    loss.backward()\n","    optimizer.step()\n","\n","  epoch_loss = np.asarray(epoch_loss)\n","  end = time.time()\n","  print(\"Epoca %d, Loss: %.4f +\\- %.4f, Tempo: %.2f\" % (epoch, epoch_loss.mean(), epoch_loss.std(), end-start) )\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":663},"executionInfo":{"elapsed":20100,"status":"ok","timestamp":1653685145271,"user":{"displayName":"Anderson P","userId":"07518979327659766490"},"user_tz":180},"id":"qht1PLLWcpls","outputId":"6fbe5456-5e7a-4e5c-b488-f8cf6835f78b"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n","--2022-05-27 20:58:45--  https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n","Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n","Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 279992 (273K) [application/x-httpd-php]\n","Saving to: ‘Bike-Sharing-Dataset.zip.1’\n","\n","Bike-Sharing-Datase 100%[===================>] 273.43K  --.-KB/s    in 0.1s    \n","\n","2022-05-27 20:58:45 (2.38 MB/s) - ‘Bike-Sharing-Dataset.zip.1’ saved [279992/279992]\n","\n","Archive:  Bike-Sharing-Dataset.zip\n","replace Readme.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: Readme.txt              \n","replace day.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: day.csv                 \n","replace hour.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: hour.csv                \n","Bike-Sharing-Dataset.zip    bike_test.csv   day.csv   MNIST\t  sample_data\n","Bike-Sharing-Dataset.zip.1  bike_train.csv  hour.csv  Readme.txt\n","17379\n"]},{"data":{"text/html":["\n","  <div id=\"df-41e8349b-b58a-4938-946e-9c62ac3bac11\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>instant</th>\n","      <th>dteday</th>\n","      <th>season</th>\n","      <th>yr</th>\n","      <th>mnth</th>\n","      <th>hr</th>\n","      <th>holiday</th>\n","      <th>weekday</th>\n","      <th>workingday</th>\n","      <th>weathersit</th>\n","      <th>temp</th>\n","      <th>atemp</th>\n","      <th>hum</th>\n","      <th>windspeed</th>\n","      <th>casual</th>\n","      <th>registered</th>\n","      <th>cnt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>2011-01-01</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.24</td>\n","      <td>0.2879</td>\n","      <td>0.81</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","      <td>13</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>2011-01-01</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.22</td>\n","      <td>0.2727</td>\n","      <td>0.80</td>\n","      <td>0.0</td>\n","      <td>8</td>\n","      <td>32</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>2011-01-01</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.22</td>\n","      <td>0.2727</td>\n","      <td>0.80</td>\n","      <td>0.0</td>\n","      <td>5</td>\n","      <td>27</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>2011-01-01</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.24</td>\n","      <td>0.2879</td>\n","      <td>0.75</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","      <td>10</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>2011-01-01</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.24</td>\n","      <td>0.2879</td>\n","      <td>0.75</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41e8349b-b58a-4938-946e-9c62ac3bac11')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-41e8349b-b58a-4938-946e-9c62ac3bac11 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-41e8349b-b58a-4938-946e-9c62ac3bac11');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n","0        1  2011-01-01       1   0     1   0        0        6           0   \n","1        2  2011-01-01       1   0     1   1        0        6           0   \n","2        3  2011-01-01       1   0     1   2        0        6           0   \n","3        4  2011-01-01       1   0     1   3        0        6           0   \n","4        5  2011-01-01       1   0     1   4        0        6           0   \n","\n","   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n","0           1  0.24  0.2879  0.81        0.0       3          13   16  \n","1           1  0.22  0.2727  0.80        0.0       8          32   40  \n","2           1  0.22  0.2727  0.80        0.0       5          27   32  \n","3           1  0.24  0.2879  0.75        0.0       3          10   13  \n","4           1  0.24  0.2879  0.75        0.0       0           1    1  "]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# Carregando dados customizados no pytorch\n","\n","import torch\n","from torch import nn, optim\n","\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from sklearn import metrics\n","from sklearn.preprocessing import StandardScaler\n","\n","import pandas as pd\n","import numpy as np\n","import time\n","import os\n","\n","# Configurando hiperparâmetros.\n","args = {\n","    'epoch_num': 200,     # Número de épocas.\n","    'lr': 5e-5,           # Taxa de aprendizado.\n","    'weight_decay': 5e-4, # Penalidade L2 (Regularização).\n","    'num_workers': 3,     # Número de threads do dataloader.\n","    'batch_size': 20,     # Tamanho do batch.\n","}\n","\n","if torch.cuda.is_available():\n","    args['device'] = torch.device('cuda')\n","else:\n","    args['device'] = torch.device('cpu')\n","\n","print(args['device'])\n","\n","# dados de bicicleata\n","# https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n","\n","! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n","! unzip Bike-Sharing-Dataset.zip  \n","! ls\n","\n","\n","df = pd.read_csv('hour.csv')\n","print(len(df))\n","df.head()\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"elapsed":743,"status":"ok","timestamp":1653685150726,"user":{"displayName":"Anderson P","userId":"07518979327659766490"},"user_tz":180},"id":"3RIAqr4OiNkX","outputId":"b50bbf12-897f-4177-cd44-fc0b3d52a910"},"outputs":[{"name":"stdout","output_type":"stream","text":["Setar os primeiros 13903 dados aleatorios em df_train\n","Setar os ultimos 3476 dados aleatorios em df_test.\n","len(df_train):13903, len(df_test):3476\n"]},{"data":{"text/html":["\n","  <div id=\"df-366cb832-d4bd-4ead-aeed-745c72702f6f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>instant</th>\n","      <th>dteday</th>\n","      <th>season</th>\n","      <th>yr</th>\n","      <th>mnth</th>\n","      <th>hr</th>\n","      <th>holiday</th>\n","      <th>weekday</th>\n","      <th>workingday</th>\n","      <th>weathersit</th>\n","      <th>temp</th>\n","      <th>atemp</th>\n","      <th>hum</th>\n","      <th>windspeed</th>\n","      <th>casual</th>\n","      <th>registered</th>\n","      <th>cnt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>12663</th>\n","      <td>12664</td>\n","      <td>2012-06-16</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>20</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0.66</td>\n","      <td>0.6212</td>\n","      <td>0.47</td>\n","      <td>0.1940</td>\n","      <td>123</td>\n","      <td>229</td>\n","      <td>352</td>\n","    </tr>\n","    <tr>\n","      <th>1801</th>\n","      <td>1802</td>\n","      <td>2011-03-20</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>18</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.38</td>\n","      <td>0.3939</td>\n","      <td>0.40</td>\n","      <td>0.3582</td>\n","      <td>58</td>\n","      <td>98</td>\n","      <td>156</td>\n","    </tr>\n","    <tr>\n","      <th>16567</th>\n","      <td>16568</td>\n","      <td>2012-11-28</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>11</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0.26</td>\n","      <td>0.2576</td>\n","      <td>0.75</td>\n","      <td>0.2239</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>8817</th>\n","      <td>8818</td>\n","      <td>2012-01-08</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0.32</td>\n","      <td>0.3333</td>\n","      <td>0.49</td>\n","      <td>0.1045</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2608</th>\n","      <td>2609</td>\n","      <td>2011-04-23</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>14</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.58</td>\n","      <td>0.5455</td>\n","      <td>0.78</td>\n","      <td>0.3582</td>\n","      <td>182</td>\n","      <td>209</td>\n","      <td>391</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-366cb832-d4bd-4ead-aeed-745c72702f6f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-366cb832-d4bd-4ead-aeed-745c72702f6f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-366cb832-d4bd-4ead-aeed-745c72702f6f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["       instant      dteday  season  yr  mnth  hr  holiday  weekday  \\\n","12663    12664  2012-06-16       2   1     6  20        0        6   \n","1801      1802  2011-03-20       1   0     3  18        0        0   \n","16567    16568  2012-11-28       4   1    11   1        0        3   \n","8817      8818  2012-01-08       1   1     1   5        0        0   \n","2608      2609  2011-04-23       2   0     4  14        0        6   \n","\n","       workingday  weathersit  temp   atemp   hum  windspeed  casual  \\\n","12663           0           2  0.66  0.6212  0.47     0.1940     123   \n","1801            0           1  0.38  0.3939  0.40     0.3582      58   \n","16567           1           2  0.26  0.2576  0.75     0.2239       0   \n","8817            0           2  0.32  0.3333  0.49     0.1045       0   \n","2608            0           1  0.58  0.5455  0.78     0.3582     182   \n","\n","       registered  cnt  \n","12663         229  352  \n","1801           98  156  \n","16567          12   12  \n","8817            2    2  \n","2608          209  391  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["ls current directory...\n","Bike-Sharing-Dataset.zip    bike_test.csv   day.csv   MNIST\t  sample_data\n","Bike-Sharing-Dataset.zip.1  bike_train.csv  hour.csv  Readme.txt\n"]}],"source":["# permuta aleatoriamente os dados\n","torch.randperm(10)\n","\n","torch.manual_seed(1)\n","# escolhe indices aleatorios para embaralhar os dados\n","indices = torch.randperm(len(df)).tolist()\n","\n","# define a porcentagem dos dados que serao dados de treino\n","train_size = int(0.8*len(df))\n","\n","# seleciona os dados de 0 até (train_size - 1) para serem dados de treino\n","print(f\"Setar os primeiros {train_size} dados aleatorios em df_train\")\n","df_train = df.iloc[indices[:train_size]]\n","# seleciona os dados de train_size até o final (len(indices) - 1)\n","print(f\"Setar os ultimos {(len(df) - train_size)} dados aleatorios em df_test.\")\n","df_test = df.iloc[indices[train_size:]]\n","\n","\n","print(\"len(df_train):\" + str(len(df_train)) + \", len(df_test):\" + str(len(df_test)))\n","display(df_test.head())\n","\n","# criar os datasets de treino e teste!\n","df_train.to_csv('bike_train.csv',index=False)\n","df_test.to_csv('bike_test.csv',index=False)\n","\n","print(\"ls current directory...\")\n","! ls\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FFcDCfoO-1qv"},"source":["Tutorial completo do PyTorch: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":693,"status":"ok","timestamp":1653685156225,"user":{"displayName":"Anderson P","userId":"07518979327659766490"},"user_tz":180},"id":"x1BLN93U9TkP","outputId":"aa980fc1-221d-4209-8b77-c5e518843e89"},"outputs":[{"name":"stdout","output_type":"stream","text":["diretório atual\n","Bike-Sharing-Dataset.zip    bike_test.csv   day.csv   MNIST\t  sample_data\n","Bike-Sharing-Dataset.zip.1  bike_train.csv  hour.csv  Readme.txt\n","rotulo:tensor([373.])\n","condições do dia: tensor([ 4.0000,  1.0000, 11.0000, 19.0000,  0.0000,  4.0000,  1.0000,  1.0000,\n","         0.3800,  0.3939,  0.2700,  0.3582])\n"]}],"source":["class BikeDataset(Dataset):\n","\n","  def __init__(self, csv_path):\n","    self.csv_data = pd.read_csv(csv_path).to_numpy()\n","\n","  def __getitem__(self, idx):\n","    # 2 ao 14 é o payload\n","    sample = self.csv_data[idx][2:14]\n","    # ultimo elemento é o label\n","    label = self.csv_data[idx][-1:]\n","    # converter para tensor\n","    # 1 converte para float32 -> 2 converte de array numpy para tensor\n","    sample = torch.from_numpy(sample.astype(np.float32))\n","    label = torch.from_numpy(label.astype(np.float32))\n","    # obrigatoriamente a saida do dataset é um tipo tupla\n","    return sample, label\n","\n","  def __len__(self):\n","    return len(self.csv_data)\n","\n","! echo \"diretório atual\"\n","! ls\n","train_set = BikeDataset(\"bike_train.csv\")\n","test_set = BikeDataset(\"bike_test.csv\")\n","\n","dado, rotulo = train_set[0]\n","print(\"rotulo:\" + str(rotulo))\n","print(\"condições do dia: \" + str(dado))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":281,"status":"ok","timestamp":1653685160146,"user":{"displayName":"Anderson P","userId":"07518979327659766490"},"user_tz":180},"id":"JnbbUM82NQ5c","outputId":"39bca33a-23ed-41ae-b67e-feb598cf9217"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"name":"stdout","output_type":"stream","text":["## Dimensionalidade do batch ##\n","torch.Size([20, 12]) torch.Size([20, 1])\n"]}],"source":["from torch.utils.data import DataLoader\n","\n","train_loader = DataLoader(train_set, \n","                          batch_size=args['batch_size'],\n","                          shuffle=True,\n","                          num_workers=args['num_workers'])\n","\n","test_loader = DataLoader(train_set, \n","                          batch_size=args['batch_size'],\n","                          shuffle=True,\n","                          num_workers=args['num_workers'])\n","\n","for batch in train_loader:\n","\n","  dado, rotulo = batch\n","  print('## Dimensionalidade do batch ##')\n","  print(dado.size(), rotulo.size())\n","  break\n","  # plt.title(\"Rotulo: \" + str(rotulo[0]))\n","  # plt.imshow(dado[0][0])\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZR6fKZ2eGYQw"},"outputs":[],"source":["# criando a rede neural\n"," \n","class MLP(nn.Module):\n","\n","  def __init__(self, input_size, hidden_size, out_size):\n","    super(MLP, self).__init__()\n","\n","    self.features  = nn.Sequential(\n","                      nn.Linear(input_size, hidden_size),\n","                      nn.ReLU(),\n","                      nn.Linear(hidden_size, hidden_size),\n","                      nn.ReLU()\n","                    )\n","    self.out     = nn.Linear(hidden_size, out_size)\n","    # retirando a ativação, pode-se permitir que a rede aprenda qualquer\n","    # intervado de valores. \n","    # o proposito do softmax é fazer com que no final exista uma probabilidade,\n","    # que não é o caso. \n","    # self.softmax = nn.Softmax()\n","\n","  def forward(self, X):\n","    # não é necessário linearizar a entrada, já que ela já\n","    # vai ter uma unica dimentsão\n","    # X = X.view(X.size(0), -1)\n","    feature = self.features(X)\n","    # não estou usando o softmax, então posso usar somente o linear, sem\n","    # ativação\n","    # output  = self.softmax(self.out(feature))\n","    output = self.out(feature)\n","\n","    return output\n","\n","\n","# train_set todas as amostras que serao testadas\n","len(train_set)\n","# train_set[0] tensor de uma tupla com o dado e o rótulo\n","train_set[0]\n","# somente o dado\n","train_set[0][0]\n","# quantos atributos temos no nosso dado\n","input_size =  len(train_set[0][0])\n","# manter o mesmo do ex anterior\n","hidden_size = 128\n","# numero de variaveis que serão preditas -> uma só variavel,\n","# quantas bicicletas serão vendidas\n","out_size = 1 \n","# Cast da rede neural na GPU se disponível\n","net = MLP(input_size, hidden_size, out_size).to(args['device'])\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"75KRL1vtJCuw"},"outputs":[],"source":["# Definindo loss e otimizador\n","\n","# loss de regressão\n","criterion = nn.L1Loss().to(args['device'])\n","\n","# é possivel utilizar o mesmo otimizador do exeplo anterior\n","optimizer = optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n","\n","\n","\n","\n","def train(train_loader, net, epoch):\n","\n","  # Training mode\n","  net.train()\n","  \n","  start = time.time()\n","  \n","  epoch_loss  = []\n","  for batch in train_loader:\n","    \n","    dado, rotulo = batch\n","    \n","    # Cast do dado na GPU\n","    dado = dado.to(args['device'])\n","    rotulo = rotulo.to(args['device'])\n","    \n","    # Forward\n","    ypred = net(dado)\n","    loss = criterion(ypred, rotulo)\n","    epoch_loss.append(loss.cpu().data)\n","    \n","    # Backpropagation\n","    loss.backward()\n","    optimizer.step()\n","   \n","  epoch_loss = np.asarray(epoch_loss)\n","  \n","  end = time.time()\n","  print('#################### Train ####################')\n","  print('Epoch %d, Loss: %.4f +/- %.4f, Time: %.2f' % (epoch, epoch_loss.mean(), epoch_loss.std(), end-start))\n","  \n","  return epoch_loss.mean()\n","    \n","\n","\n","def validate(test_loader, net, epoch):\n","\n","  # Evaluation mode\n","  net.eval()\n","  \n","  start = time.time()\n","  \n","  epoch_loss  = []\n","  \n","  with torch.no_grad(): \n","    for batch in test_loader:\n","\n","      dado, rotulo = batch\n","\n","      # Cast do dado na GPU\n","      dado = dado.to(args['device'])\n","      rotulo = rotulo.to(args['device'])\n","\n","      # Forward\n","      ypred = net(dado)\n","      loss = criterion(ypred, rotulo)\n","      epoch_loss.append(loss.cpu().data)\n","\n","  epoch_loss = np.asarray(epoch_loss)\n","  \n","  end = time.time()\n","  print('********** Validate **********')\n","  print('Epoch %d, Loss: %.4f +/- %.4f, Time: %.2f\\n' % (epoch, epoch_loss.mean(), epoch_loss.std(), end-start))\n","  \n","  return epoch_loss.mean()\n","    \n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41005,"status":"ok","timestamp":1653685340940,"user":{"displayName":"Anderson P","userId":"07518979327659766490"},"user_tz":180},"id":"1K93hK6GQP06","outputId":"1c24027f-6620-4529-ed16-c5de782c5004"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"name":"stdout","output_type":"stream","text":["#################### Train ####################\n","Epoch 0, Loss: 162.1998 +/- 41.4528, Time: 2.62\n","********** Validate **********\n","Epoch 0, Loss: 125.0877 +/- 32.2792, Time: 2.60\n","\n","#################### Train ####################\n","Epoch 1, Loss: 131.7140 +/- 24.9943, Time: 2.48\n","********** Validate **********\n","Epoch 1, Loss: 126.6249 +/- 24.3527, Time: 1.81\n","\n","#################### Train ####################\n","Epoch 2, Loss: 122.8945 +/- 31.4290, Time: 2.62\n","********** Validate **********\n","Epoch 2, Loss: 129.4849 +/- 32.9525, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 3, Loss: 123.1575 +/- 31.6277, Time: 2.52\n","********** Validate **********\n","Epoch 3, Loss: 116.4208 +/- 28.3660, Time: 1.76\n","\n","#################### Train ####################\n","Epoch 4, Loss: 121.8102 +/- 25.1997, Time: 2.60\n","********** Validate **********\n","Epoch 4, Loss: 117.1145 +/- 26.7825, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 5, Loss: 119.9313 +/- 29.0956, Time: 2.67\n","********** Validate **********\n","Epoch 5, Loss: 122.4281 +/- 31.1169, Time: 1.76\n","\n","#################### Train ####################\n","Epoch 6, Loss: 117.8738 +/- 29.1262, Time: 2.70\n","********** Validate **********\n","Epoch 6, Loss: 119.1156 +/- 26.0399, Time: 1.83\n","\n","#################### Train ####################\n","Epoch 7, Loss: 115.7579 +/- 27.8495, Time: 2.76\n","********** Validate **********\n","Epoch 7, Loss: 115.2908 +/- 30.8972, Time: 1.82\n","\n","#################### Train ####################\n","Epoch 8, Loss: 114.3157 +/- 28.9259, Time: 2.83\n","********** Validate **********\n","Epoch 8, Loss: 110.9778 +/- 25.9789, Time: 1.86\n","\n","#################### Train ####################\n","Epoch 9, Loss: 110.7355 +/- 26.1801, Time: 2.91\n","********** Validate **********\n","Epoch 9, Loss: 108.6196 +/- 27.0676, Time: 1.90\n","\n","#################### Train ####################\n","Epoch 10, Loss: 108.8995 +/- 28.0583, Time: 3.01\n","********** Validate **********\n","Epoch 10, Loss: 106.8290 +/- 24.9545, Time: 1.84\n","\n","#################### Train ####################\n","Epoch 11, Loss: 105.0990 +/- 23.9888, Time: 3.09\n","********** Validate **********\n","Epoch 11, Loss: 103.5622 +/- 26.8332, Time: 1.80\n","\n","#################### Train ####################\n","Epoch 12, Loss: 101.3357 +/- 26.6080, Time: 3.18\n","********** Validate **********\n","Epoch 12, Loss: 99.0902 +/- 23.4149, Time: 1.79\n","\n","#################### Train ####################\n","Epoch 13, Loss: 97.9242 +/- 24.8452, Time: 3.04\n","********** Validate **********\n","Epoch 13, Loss: 97.0676 +/- 23.7640, Time: 1.80\n","\n","#################### Train ####################\n","Epoch 14, Loss: 96.5025 +/- 23.6350, Time: 3.07\n","********** Validate **********\n","Epoch 14, Loss: 95.0417 +/- 22.9124, Time: 1.84\n","\n","#################### Train ####################\n","Epoch 15, Loss: 93.7567 +/- 24.4728, Time: 3.04\n","********** Validate **********\n","Epoch 15, Loss: 92.0607 +/- 23.2063, Time: 1.88\n","\n","#################### Train ####################\n","Epoch 16, Loss: 91.7100 +/- 23.3680, Time: 2.96\n","********** Validate **********\n","Epoch 16, Loss: 90.4549 +/- 23.0258, Time: 1.80\n","\n","#################### Train ####################\n","Epoch 17, Loss: 90.6623 +/- 23.2371, Time: 2.99\n","********** Validate **********\n","Epoch 17, Loss: 90.6683 +/- 20.8999, Time: 1.79\n","\n","#################### Train ####################\n","Epoch 18, Loss: 90.2802 +/- 22.9121, Time: 2.94\n","********** Validate **********\n","Epoch 18, Loss: 90.8649 +/- 22.1468, Time: 1.88\n","\n","#################### Train ####################\n","Epoch 19, Loss: 89.3260 +/- 22.9595, Time: 2.86\n","********** Validate **********\n","Epoch 19, Loss: 88.0093 +/- 21.5860, Time: 1.76\n","\n","#################### Train ####################\n","Epoch 20, Loss: 88.0841 +/- 22.7621, Time: 2.84\n","********** Validate **********\n","Epoch 20, Loss: 88.1226 +/- 22.8533, Time: 1.82\n","\n","#################### Train ####################\n","Epoch 21, Loss: 88.3405 +/- 22.6729, Time: 2.75\n","********** Validate **********\n","Epoch 21, Loss: 89.0657 +/- 24.7085, Time: 1.82\n","\n","#################### Train ####################\n","Epoch 22, Loss: 88.4619 +/- 22.6396, Time: 2.75\n","********** Validate **********\n","Epoch 22, Loss: 86.1678 +/- 24.6795, Time: 1.78\n","\n","#################### Train ####################\n","Epoch 23, Loss: 88.0154 +/- 22.2661, Time: 2.70\n","********** Validate **********\n","Epoch 23, Loss: 85.4924 +/- 23.7025, Time: 1.85\n","\n","#################### Train ####################\n","Epoch 24, Loss: 87.9209 +/- 22.4799, Time: 2.73\n","********** Validate **********\n","Epoch 24, Loss: 84.7761 +/- 23.6370, Time: 1.78\n","\n","#################### Train ####################\n","Epoch 25, Loss: 87.4308 +/- 22.0732, Time: 2.75\n","********** Validate **********\n","Epoch 25, Loss: 83.4892 +/- 22.7119, Time: 1.71\n","\n","#################### Train ####################\n","Epoch 26, Loss: 88.1854 +/- 23.6395, Time: 2.72\n","********** Validate **********\n","Epoch 26, Loss: 83.9657 +/- 22.9620, Time: 1.76\n","\n","#################### Train ####################\n","Epoch 27, Loss: 88.8069 +/- 23.6675, Time: 2.74\n","********** Validate **********\n","Epoch 27, Loss: 91.4823 +/- 26.2280, Time: 1.74\n","\n","#################### Train ####################\n","Epoch 28, Loss: 88.4559 +/- 23.2578, Time: 2.69\n","********** Validate **********\n","Epoch 28, Loss: 91.9418 +/- 26.1639, Time: 1.83\n","\n","#################### Train ####################\n","Epoch 29, Loss: 90.4345 +/- 23.1367, Time: 2.66\n","********** Validate **********\n","Epoch 29, Loss: 87.7040 +/- 20.5164, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 30, Loss: 87.1062 +/- 23.9901, Time: 2.69\n","********** Validate **********\n","Epoch 30, Loss: 85.2032 +/- 20.3238, Time: 1.81\n","\n","#################### Train ####################\n","Epoch 31, Loss: 87.6250 +/- 23.0177, Time: 2.72\n","********** Validate **********\n","Epoch 31, Loss: 88.7796 +/- 26.2690, Time: 1.79\n","\n","#################### Train ####################\n","Epoch 32, Loss: 85.5009 +/- 22.6257, Time: 2.68\n","********** Validate **********\n","Epoch 32, Loss: 81.8177 +/- 21.7519, Time: 1.76\n","\n","#################### Train ####################\n","Epoch 33, Loss: 83.6044 +/- 24.0738, Time: 2.73\n","********** Validate **********\n","Epoch 33, Loss: 84.7349 +/- 20.6119, Time: 1.77\n","\n","#################### Train ####################\n","Epoch 34, Loss: 84.6738 +/- 23.2151, Time: 2.73\n","********** Validate **********\n","Epoch 34, Loss: 86.2415 +/- 24.6764, Time: 1.72\n","\n","#################### Train ####################\n","Epoch 35, Loss: 83.7508 +/- 22.0732, Time: 2.74\n","********** Validate **********\n","Epoch 35, Loss: 80.6727 +/- 22.0563, Time: 1.79\n","\n","#################### Train ####################\n","Epoch 36, Loss: 83.7860 +/- 22.9600, Time: 2.71\n","********** Validate **********\n","Epoch 36, Loss: 85.7747 +/- 20.1550, Time: 1.73\n","\n","#################### Train ####################\n","Epoch 37, Loss: 84.2038 +/- 22.9202, Time: 2.70\n","********** Validate **********\n","Epoch 37, Loss: 84.4364 +/- 25.9975, Time: 1.88\n","\n","#################### Train ####################\n","Epoch 38, Loss: 83.5333 +/- 21.6486, Time: 2.63\n","********** Validate **********\n","Epoch 38, Loss: 80.3115 +/- 21.4684, Time: 1.73\n","\n","#################### Train ####################\n","Epoch 39, Loss: 83.0452 +/- 23.8278, Time: 2.54\n","********** Validate **********\n","Epoch 39, Loss: 80.8992 +/- 20.7867, Time: 1.70\n","\n","#################### Train ####################\n","Epoch 40, Loss: 82.6806 +/- 21.9664, Time: 2.64\n","********** Validate **********\n","Epoch 40, Loss: 83.4471 +/- 24.4245, Time: 1.73\n","\n","#################### Train ####################\n","Epoch 41, Loss: 82.5018 +/- 21.4632, Time: 2.70\n","********** Validate **********\n","Epoch 41, Loss: 83.2179 +/- 21.4269, Time: 1.71\n","\n","#################### Train ####################\n","Epoch 42, Loss: 81.6046 +/- 22.8453, Time: 2.61\n","********** Validate **********\n","Epoch 42, Loss: 79.3672 +/- 23.7976, Time: 1.69\n","\n","#################### Train ####################\n","Epoch 43, Loss: 80.3141 +/- 20.8746, Time: 2.61\n","********** Validate **********\n","Epoch 43, Loss: 79.5711 +/- 23.9342, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 44, Loss: 80.0451 +/- 22.0261, Time: 2.63\n","********** Validate **********\n","Epoch 44, Loss: 81.4577 +/- 20.1740, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 45, Loss: 79.9863 +/- 20.8048, Time: 2.69\n","********** Validate **********\n","Epoch 45, Loss: 77.3065 +/- 21.1547, Time: 1.68\n","\n","#################### Train ####################\n","Epoch 46, Loss: 79.2817 +/- 21.3370, Time: 2.60\n","********** Validate **********\n","Epoch 46, Loss: 80.7524 +/- 24.2066, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 47, Loss: 79.8852 +/- 22.0005, Time: 2.77\n","********** Validate **********\n","Epoch 47, Loss: 78.6695 +/- 20.5338, Time: 1.73\n","\n","#################### Train ####################\n","Epoch 48, Loss: 78.9525 +/- 23.4632, Time: 2.71\n","********** Validate **********\n","Epoch 48, Loss: 78.3715 +/- 20.5941, Time: 1.78\n","\n","#################### Train ####################\n","Epoch 49, Loss: 79.9128 +/- 22.7118, Time: 2.72\n","********** Validate **********\n","Epoch 49, Loss: 82.7392 +/- 25.3401, Time: 1.78\n","\n","#################### Train ####################\n","Epoch 50, Loss: 79.3519 +/- 21.4091, Time: 2.64\n","********** Validate **********\n","Epoch 50, Loss: 79.3286 +/- 20.1991, Time: 1.68\n","\n","#################### Train ####################\n","Epoch 51, Loss: 78.6677 +/- 23.2681, Time: 2.66\n","********** Validate **********\n","Epoch 51, Loss: 75.7387 +/- 21.0733, Time: 1.70\n","\n","#################### Train ####################\n","Epoch 52, Loss: 78.1443 +/- 21.4736, Time: 2.67\n","********** Validate **********\n","Epoch 52, Loss: 77.9794 +/- 23.1450, Time: 1.69\n","\n","#################### Train ####################\n","Epoch 53, Loss: 78.5052 +/- 22.3771, Time: 2.62\n","********** Validate **********\n","Epoch 53, Loss: 80.5595 +/- 19.9169, Time: 1.72\n","\n","#################### Train ####################\n","Epoch 54, Loss: 77.9126 +/- 21.5275, Time: 2.62\n","********** Validate **********\n","Epoch 54, Loss: 78.0293 +/- 22.9639, Time: 1.69\n","\n","#################### Train ####################\n","Epoch 55, Loss: 76.4677 +/- 20.9412, Time: 2.64\n","********** Validate **********\n","Epoch 55, Loss: 74.3230 +/- 20.4634, Time: 1.74\n","\n","#################### Train ####################\n","Epoch 56, Loss: 76.2338 +/- 21.6394, Time: 2.65\n","********** Validate **********\n","Epoch 56, Loss: 76.5519 +/- 20.0899, Time: 1.72\n","\n","#################### Train ####################\n","Epoch 57, Loss: 76.3232 +/- 22.2542, Time: 2.63\n","********** Validate **********\n","Epoch 57, Loss: 75.6622 +/- 22.7987, Time: 1.73\n","\n","#################### Train ####################\n","Epoch 58, Loss: 75.2489 +/- 20.8140, Time: 2.66\n","********** Validate **********\n","Epoch 58, Loss: 74.9514 +/- 22.7159, Time: 1.76\n","\n","#################### Train ####################\n","Epoch 59, Loss: 75.2094 +/- 20.9515, Time: 2.84\n","********** Validate **********\n","Epoch 59, Loss: 77.6221 +/- 19.0001, Time: 1.76\n","\n","#################### Train ####################\n","Epoch 60, Loss: 75.5130 +/- 21.9928, Time: 2.65\n","********** Validate **********\n","Epoch 60, Loss: 72.8846 +/- 20.3117, Time: 1.77\n","\n","#################### Train ####################\n","Epoch 61, Loss: 74.9631 +/- 20.1296, Time: 2.69\n","********** Validate **********\n","Epoch 61, Loss: 77.1121 +/- 24.4120, Time: 1.76\n","\n","#################### Train ####################\n","Epoch 62, Loss: 75.3932 +/- 20.9561, Time: 2.73\n","********** Validate **********\n","Epoch 62, Loss: 72.5605 +/- 20.9439, Time: 1.81\n","\n","#################### Train ####################\n","Epoch 63, Loss: 75.1408 +/- 21.6468, Time: 2.80\n","********** Validate **********\n","Epoch 63, Loss: 77.9803 +/- 19.8185, Time: 1.79\n","\n","#################### Train ####################\n","Epoch 64, Loss: 76.0189 +/- 21.6939, Time: 2.72\n","********** Validate **********\n","Epoch 64, Loss: 74.5384 +/- 23.1310, Time: 1.80\n","\n","#################### Train ####################\n","Epoch 65, Loss: 74.0607 +/- 20.6181, Time: 2.69\n","********** Validate **********\n","Epoch 65, Loss: 73.4238 +/- 20.5870, Time: 1.72\n","\n","#################### Train ####################\n","Epoch 66, Loss: 75.5877 +/- 20.3363, Time: 2.82\n","********** Validate **********\n","Epoch 66, Loss: 78.7207 +/- 18.9606, Time: 1.78\n","\n","#################### Train ####################\n","Epoch 67, Loss: 76.5130 +/- 21.4778, Time: 3.70\n","********** Validate **********\n","Epoch 67, Loss: 76.8485 +/- 24.0317, Time: 2.15\n","\n","#################### Train ####################\n","Epoch 68, Loss: 74.9983 +/- 19.3925, Time: 4.08\n","********** Validate **********\n","Epoch 68, Loss: 72.5602 +/- 18.8149, Time: 2.52\n","\n","#################### Train ####################\n","Epoch 69, Loss: 74.3209 +/- 21.9551, Time: 2.88\n","********** Validate **********\n","Epoch 69, Loss: 73.6691 +/- 19.1499, Time: 1.79\n","\n","#################### Train ####################\n","Epoch 70, Loss: 74.7316 +/- 19.6767, Time: 2.96\n","********** Validate **********\n","Epoch 70, Loss: 77.0369 +/- 21.5250, Time: 1.82\n","\n","#################### Train ####################\n","Epoch 71, Loss: 75.0949 +/- 20.3402, Time: 2.75\n","********** Validate **********\n","Epoch 71, Loss: 73.7424 +/- 18.4435, Time: 1.81\n","\n","#################### Train ####################\n","Epoch 72, Loss: 73.5959 +/- 21.6080, Time: 2.85\n","********** Validate **********\n","Epoch 72, Loss: 71.1678 +/- 19.7245, Time: 1.84\n","\n","#################### Train ####################\n","Epoch 73, Loss: 73.4005 +/- 20.1815, Time: 2.82\n","********** Validate **********\n","Epoch 73, Loss: 75.9440 +/- 22.1930, Time: 1.82\n","\n","#################### Train ####################\n","Epoch 74, Loss: 74.0853 +/- 21.0559, Time: 2.75\n","********** Validate **********\n","Epoch 74, Loss: 74.1857 +/- 18.5826, Time: 1.70\n","\n","#################### Train ####################\n","Epoch 75, Loss: 73.0918 +/- 21.2872, Time: 2.79\n","********** Validate **********\n","Epoch 75, Loss: 70.2353 +/- 19.5547, Time: 1.77\n","\n","#################### Train ####################\n","Epoch 76, Loss: 72.6945 +/- 18.8331, Time: 2.87\n","********** Validate **********\n","Epoch 76, Loss: 74.5490 +/- 21.6728, Time: 1.73\n","\n","#################### Train ####################\n","Epoch 77, Loss: 72.7230 +/- 19.5357, Time: 2.76\n","********** Validate **********\n","Epoch 77, Loss: 72.8890 +/- 18.2249, Time: 1.68\n","\n","#################### Train ####################\n","Epoch 78, Loss: 72.4833 +/- 21.2560, Time: 2.75\n","********** Validate **********\n","Epoch 78, Loss: 69.8887 +/- 19.6891, Time: 1.76\n","\n","#################### Train ####################\n","Epoch 79, Loss: 71.9241 +/- 18.7718, Time: 2.85\n","********** Validate **********\n","Epoch 79, Loss: 74.3279 +/- 22.9265, Time: 1.84\n","\n","#################### Train ####################\n","Epoch 80, Loss: 72.3978 +/- 19.0011, Time: 2.79\n","********** Validate **********\n","Epoch 80, Loss: 72.6370 +/- 17.6867, Time: 1.80\n","\n","#################### Train ####################\n","Epoch 81, Loss: 71.4920 +/- 20.5389, Time: 2.72\n","********** Validate **********\n","Epoch 81, Loss: 69.0172 +/- 18.7728, Time: 1.71\n","\n","#################### Train ####################\n","Epoch 82, Loss: 71.5830 +/- 20.0781, Time: 2.76\n","********** Validate **********\n","Epoch 82, Loss: 73.3541 +/- 21.9459, Time: 1.70\n","\n","#################### Train ####################\n","Epoch 83, Loss: 71.2204 +/- 19.8166, Time: 2.82\n","********** Validate **********\n","Epoch 83, Loss: 71.3856 +/- 18.1659, Time: 1.68\n","\n","#################### Train ####################\n","Epoch 84, Loss: 70.2224 +/- 21.2956, Time: 2.72\n","********** Validate **********\n","Epoch 84, Loss: 68.0115 +/- 19.1283, Time: 1.77\n","\n","#################### Train ####################\n","Epoch 85, Loss: 70.1273 +/- 19.0018, Time: 2.80\n","********** Validate **********\n","Epoch 85, Loss: 71.9594 +/- 21.2276, Time: 1.69\n","\n","#################### Train ####################\n","Epoch 86, Loss: 69.7937 +/- 18.7983, Time: 2.79\n","********** Validate **********\n","Epoch 86, Loss: 67.7750 +/- 18.8142, Time: 1.87\n","\n","#################### Train ####################\n","Epoch 87, Loss: 69.1190 +/- 20.9100, Time: 2.79\n","********** Validate **********\n","Epoch 87, Loss: 70.1047 +/- 17.2965, Time: 1.68\n","\n","#################### Train ####################\n","Epoch 88, Loss: 68.6281 +/- 19.8196, Time: 2.77\n","********** Validate **********\n","Epoch 88, Loss: 67.7470 +/- 20.0289, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 89, Loss: 68.3477 +/- 19.6117, Time: 2.80\n","********** Validate **********\n","Epoch 89, Loss: 69.5278 +/- 20.0797, Time: 1.81\n","\n","#################### Train ####################\n","Epoch 90, Loss: 68.3440 +/- 19.3727, Time: 2.82\n","********** Validate **********\n","Epoch 90, Loss: 66.6153 +/- 19.1205, Time: 1.74\n","\n","#################### Train ####################\n","Epoch 91, Loss: 68.1083 +/- 19.3306, Time: 2.82\n","********** Validate **********\n","Epoch 91, Loss: 70.7850 +/- 17.3066, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 92, Loss: 67.9424 +/- 19.1199, Time: 2.80\n","********** Validate **********\n","Epoch 92, Loss: 66.9753 +/- 19.9767, Time: 1.70\n","\n","#################### Train ####################\n","Epoch 93, Loss: 67.4220 +/- 18.0672, Time: 2.82\n","********** Validate **********\n","Epoch 93, Loss: 68.6528 +/- 20.4335, Time: 1.71\n","\n","#################### Train ####################\n","Epoch 94, Loss: 68.0714 +/- 18.5856, Time: 2.80\n","********** Validate **********\n","Epoch 94, Loss: 67.9487 +/- 17.3802, Time: 1.73\n","\n","#################### Train ####################\n","Epoch 95, Loss: 67.1841 +/- 19.0451, Time: 2.90\n","********** Validate **********\n","Epoch 95, Loss: 66.8182 +/- 17.0249, Time: 1.74\n","\n","#################### Train ####################\n","Epoch 96, Loss: 67.6716 +/- 17.6497, Time: 2.85\n","********** Validate **********\n","Epoch 96, Loss: 69.0059 +/- 20.1215, Time: 1.70\n","\n","#################### Train ####################\n","Epoch 97, Loss: 67.2994 +/- 18.7048, Time: 2.82\n","********** Validate **********\n","Epoch 97, Loss: 64.7879 +/- 18.3281, Time: 1.73\n","\n","#################### Train ####################\n","Epoch 98, Loss: 66.8179 +/- 18.9932, Time: 2.75\n","********** Validate **********\n","Epoch 98, Loss: 68.2479 +/- 18.0537, Time: 1.69\n","\n","#################### Train ####################\n","Epoch 99, Loss: 66.5884 +/- 19.1905, Time: 2.83\n","********** Validate **********\n","Epoch 99, Loss: 66.2712 +/- 19.2629, Time: 1.72\n","\n","#################### Train ####################\n","Epoch 100, Loss: 65.8784 +/- 18.5142, Time: 2.86\n","********** Validate **********\n","Epoch 100, Loss: 64.7973 +/- 19.5293, Time: 1.79\n","\n","#################### Train ####################\n","Epoch 101, Loss: 65.8907 +/- 18.8369, Time: 2.82\n","********** Validate **********\n","Epoch 101, Loss: 67.8778 +/- 17.1497, Time: 1.67\n","\n","#################### Train ####################\n","Epoch 102, Loss: 65.4982 +/- 18.1872, Time: 2.81\n","********** Validate **********\n","Epoch 102, Loss: 63.9022 +/- 17.1100, Time: 1.82\n","\n","#################### Train ####################\n","Epoch 103, Loss: 65.2793 +/- 18.8190, Time: 2.84\n","********** Validate **********\n","Epoch 103, Loss: 67.4688 +/- 20.1236, Time: 1.72\n","\n","#################### Train ####################\n","Epoch 104, Loss: 65.9973 +/- 16.8804, Time: 2.77\n","********** Validate **********\n","Epoch 104, Loss: 64.3840 +/- 16.4366, Time: 1.80\n","\n","#################### Train ####################\n","Epoch 105, Loss: 64.8099 +/- 19.1565, Time: 2.83\n","********** Validate **********\n","Epoch 105, Loss: 64.4567 +/- 16.9132, Time: 1.72\n","\n","#################### Train ####################\n","Epoch 106, Loss: 64.7122 +/- 17.5415, Time: 2.78\n","********** Validate **********\n","Epoch 106, Loss: 65.0130 +/- 18.7573, Time: 1.83\n","\n","#################### Train ####################\n","Epoch 107, Loss: 64.3944 +/- 17.9755, Time: 2.87\n","********** Validate **********\n","Epoch 107, Loss: 63.9975 +/- 18.4336, Time: 1.73\n","\n","#################### Train ####################\n","Epoch 108, Loss: 64.4941 +/- 17.4981, Time: 2.86\n","********** Validate **********\n","Epoch 108, Loss: 65.3413 +/- 16.6279, Time: 1.77\n","\n","#################### Train ####################\n","Epoch 109, Loss: 63.9730 +/- 17.7038, Time: 2.98\n","********** Validate **********\n","Epoch 109, Loss: 62.9839 +/- 19.2875, Time: 1.71\n","\n","#################### Train ####################\n","Epoch 110, Loss: 63.4734 +/- 16.6454, Time: 2.85\n","********** Validate **********\n","Epoch 110, Loss: 63.6866 +/- 18.2158, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 111, Loss: 62.7125 +/- 17.6449, Time: 2.87\n","********** Validate **********\n","Epoch 111, Loss: 62.4814 +/- 18.5431, Time: 1.73\n","\n","#################### Train ####################\n","Epoch 112, Loss: 62.9105 +/- 17.1969, Time: 2.83\n","********** Validate **********\n","Epoch 112, Loss: 61.9460 +/- 16.3177, Time: 1.80\n","\n","#################### Train ####################\n","Epoch 113, Loss: 62.7689 +/- 16.7838, Time: 2.91\n","********** Validate **********\n","Epoch 113, Loss: 62.1726 +/- 17.7192, Time: 1.69\n","\n","#################### Train ####################\n","Epoch 114, Loss: 62.0949 +/- 17.5547, Time: 2.85\n","********** Validate **********\n","Epoch 114, Loss: 62.6104 +/- 16.8339, Time: 1.78\n","\n","#################### Train ####################\n","Epoch 115, Loss: 61.9360 +/- 17.3791, Time: 2.85\n","********** Validate **********\n","Epoch 115, Loss: 62.2383 +/- 16.5647, Time: 1.84\n","\n","#################### Train ####################\n","Epoch 116, Loss: 62.1117 +/- 16.8473, Time: 2.99\n","********** Validate **********\n","Epoch 116, Loss: 61.1520 +/- 17.6522, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 117, Loss: 62.2232 +/- 17.6470, Time: 2.96\n","********** Validate **********\n","Epoch 117, Loss: 61.8345 +/- 18.1605, Time: 1.74\n","\n","#################### Train ####################\n","Epoch 118, Loss: 61.9014 +/- 17.9928, Time: 2.86\n","********** Validate **********\n","Epoch 118, Loss: 63.0840 +/- 18.7075, Time: 1.65\n","\n","#################### Train ####################\n","Epoch 119, Loss: 61.9343 +/- 18.3995, Time: 2.79\n","********** Validate **********\n","Epoch 119, Loss: 62.3847 +/- 17.0350, Time: 1.85\n","\n","#################### Train ####################\n","Epoch 120, Loss: 61.6719 +/- 17.6085, Time: 2.79\n","********** Validate **********\n","Epoch 120, Loss: 61.1406 +/- 17.0655, Time: 1.72\n","\n","#################### Train ####################\n","Epoch 121, Loss: 61.2619 +/- 17.3876, Time: 2.84\n","********** Validate **********\n","Epoch 121, Loss: 61.2633 +/- 16.7205, Time: 1.78\n","\n","#################### Train ####################\n","Epoch 122, Loss: 61.3253 +/- 18.1161, Time: 2.86\n","********** Validate **********\n","Epoch 122, Loss: 62.2237 +/- 16.6981, Time: 1.77\n","\n","#################### Train ####################\n","Epoch 123, Loss: 61.2222 +/- 17.9112, Time: 2.87\n","********** Validate **********\n","Epoch 123, Loss: 61.8610 +/- 16.3382, Time: 1.73\n","\n","#################### Train ####################\n","Epoch 124, Loss: 60.9536 +/- 16.8204, Time: 2.84\n","********** Validate **********\n","Epoch 124, Loss: 60.2310 +/- 16.9628, Time: 1.76\n","\n","#################### Train ####################\n","Epoch 125, Loss: 60.7778 +/- 16.9073, Time: 2.99\n","********** Validate **********\n","Epoch 125, Loss: 59.9656 +/- 17.4776, Time: 1.76\n","\n","#################### Train ####################\n","Epoch 126, Loss: 60.2827 +/- 16.8450, Time: 3.06\n","********** Validate **********\n","Epoch 126, Loss: 59.7602 +/- 17.5035, Time: 1.81\n","\n","#################### Train ####################\n","Epoch 127, Loss: 59.7403 +/- 16.1344, Time: 2.88\n","********** Validate **********\n","Epoch 127, Loss: 59.4754 +/- 17.3665, Time: 1.78\n","\n","#################### Train ####################\n","Epoch 128, Loss: 59.4410 +/- 16.6157, Time: 2.92\n","********** Validate **********\n","Epoch 128, Loss: 59.9118 +/- 17.9978, Time: 1.77\n","\n","#################### Train ####################\n","Epoch 129, Loss: 59.5351 +/- 17.0680, Time: 2.96\n","********** Validate **********\n","Epoch 129, Loss: 60.1783 +/- 17.5499, Time: 1.72\n","\n","#################### Train ####################\n","Epoch 130, Loss: 59.6126 +/- 16.8635, Time: 2.79\n","********** Validate **********\n","Epoch 130, Loss: 58.9783 +/- 16.1757, Time: 1.77\n","\n","#################### Train ####################\n","Epoch 131, Loss: 59.3898 +/- 16.1631, Time: 2.95\n","********** Validate **********\n","Epoch 131, Loss: 58.0862 +/- 15.8575, Time: 1.77\n","\n","#################### Train ####################\n","Epoch 132, Loss: 59.6927 +/- 15.8071, Time: 2.86\n","********** Validate **********\n","Epoch 132, Loss: 59.3704 +/- 15.1279, Time: 1.73\n","\n","#################### Train ####################\n","Epoch 133, Loss: 59.4952 +/- 17.1450, Time: 2.83\n","********** Validate **********\n","Epoch 133, Loss: 61.7824 +/- 16.4849, Time: 1.73\n","\n","#################### Train ####################\n","Epoch 134, Loss: 60.5325 +/- 18.0897, Time: 2.83\n","********** Validate **********\n","Epoch 134, Loss: 57.6658 +/- 16.7804, Time: 1.70\n","\n","#################### Train ####################\n","Epoch 135, Loss: 60.2719 +/- 16.9099, Time: 2.91\n","********** Validate **********\n","Epoch 135, Loss: 62.8779 +/- 19.0785, Time: 1.64\n","\n","#################### Train ####################\n","Epoch 136, Loss: 59.8129 +/- 16.7268, Time: 2.92\n","********** Validate **********\n","Epoch 136, Loss: 57.7823 +/- 15.3779, Time: 1.71\n","\n","#################### Train ####################\n","Epoch 137, Loss: 59.9379 +/- 17.7642, Time: 2.84\n","********** Validate **********\n","Epoch 137, Loss: 60.4322 +/- 15.5362, Time: 1.73\n","\n","#################### Train ####################\n","Epoch 138, Loss: 60.0017 +/- 16.9034, Time: 3.00\n","********** Validate **********\n","Epoch 138, Loss: 61.3717 +/- 18.0007, Time: 1.77\n","\n","#################### Train ####################\n","Epoch 139, Loss: 59.5981 +/- 15.9700, Time: 2.87\n","********** Validate **********\n","Epoch 139, Loss: 56.7979 +/- 16.1699, Time: 1.73\n","\n","#################### Train ####################\n","Epoch 140, Loss: 59.3291 +/- 17.5438, Time: 2.94\n","********** Validate **********\n","Epoch 140, Loss: 61.7322 +/- 16.0407, Time: 1.67\n","\n","#################### Train ####################\n","Epoch 141, Loss: 60.3789 +/- 17.1635, Time: 2.76\n","********** Validate **********\n","Epoch 141, Loss: 61.2652 +/- 17.5083, Time: 1.74\n","\n","#################### Train ####################\n","Epoch 142, Loss: 58.6673 +/- 15.4764, Time: 2.98\n","********** Validate **********\n","Epoch 142, Loss: 56.8311 +/- 15.6752, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 143, Loss: 58.7935 +/- 17.5594, Time: 2.76\n","********** Validate **********\n","Epoch 143, Loss: 59.2985 +/- 15.4154, Time: 1.66\n","\n","#################### Train ####################\n","Epoch 144, Loss: 59.1017 +/- 16.3637, Time: 2.85\n","********** Validate **********\n","Epoch 144, Loss: 58.4869 +/- 16.8111, Time: 1.70\n","\n","#################### Train ####################\n","Epoch 145, Loss: 57.5209 +/- 16.9343, Time: 2.81\n","********** Validate **********\n","Epoch 145, Loss: 56.5223 +/- 15.3543, Time: 1.80\n","\n","#################### Train ####################\n","Epoch 146, Loss: 57.9400 +/- 15.7433, Time: 2.86\n","********** Validate **********\n","Epoch 146, Loss: 59.2095 +/- 16.1572, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 147, Loss: 58.2386 +/- 16.7705, Time: 2.88\n","********** Validate **********\n","Epoch 147, Loss: 56.9326 +/- 16.0236, Time: 1.71\n","\n","#################### Train ####################\n","Epoch 148, Loss: 58.1485 +/- 15.7269, Time: 2.85\n","********** Validate **********\n","Epoch 148, Loss: 59.0904 +/- 16.5933, Time: 1.69\n","\n","#################### Train ####################\n","Epoch 149, Loss: 58.0199 +/- 16.1542, Time: 2.86\n","********** Validate **********\n","Epoch 149, Loss: 57.0211 +/- 15.3289, Time: 1.76\n","\n","#################### Train ####################\n","Epoch 150, Loss: 57.5085 +/- 16.8006, Time: 2.80\n","********** Validate **********\n","Epoch 150, Loss: 58.5061 +/- 15.8437, Time: 1.72\n","\n","#################### Train ####################\n","Epoch 151, Loss: 58.1367 +/- 15.7168, Time: 2.97\n","********** Validate **********\n","Epoch 151, Loss: 58.2206 +/- 15.6782, Time: 1.80\n","\n","#################### Train ####################\n","Epoch 152, Loss: 57.3055 +/- 15.3987, Time: 2.94\n","********** Validate **********\n","Epoch 152, Loss: 57.1295 +/- 16.1032, Time: 1.70\n","\n","#################### Train ####################\n","Epoch 153, Loss: 57.5678 +/- 15.8741, Time: 2.98\n","********** Validate **********\n","Epoch 153, Loss: 58.6614 +/- 15.5561, Time: 1.81\n","\n","#################### Train ####################\n","Epoch 154, Loss: 57.1294 +/- 16.0040, Time: 2.85\n","********** Validate **********\n","Epoch 154, Loss: 57.3472 +/- 16.0042, Time: 1.79\n","\n","#################### Train ####################\n","Epoch 155, Loss: 57.2570 +/- 16.8235, Time: 2.94\n","********** Validate **********\n","Epoch 155, Loss: 58.5007 +/- 16.3522, Time: 1.78\n","\n","#################### Train ####################\n","Epoch 156, Loss: 56.8106 +/- 15.1970, Time: 2.85\n","********** Validate **********\n","Epoch 156, Loss: 56.4019 +/- 16.1054, Time: 1.71\n","\n","#################### Train ####################\n","Epoch 157, Loss: 57.3802 +/- 16.1486, Time: 2.85\n","********** Validate **********\n","Epoch 157, Loss: 56.5785 +/- 15.5494, Time: 1.74\n","\n","#################### Train ####################\n","Epoch 158, Loss: 56.9565 +/- 15.8350, Time: 2.84\n","********** Validate **********\n","Epoch 158, Loss: 55.3040 +/- 15.1317, Time: 1.71\n","\n","#################### Train ####################\n","Epoch 159, Loss: 56.6960 +/- 15.6939, Time: 2.81\n","********** Validate **********\n","Epoch 159, Loss: 56.6437 +/- 17.0308, Time: 1.74\n","\n","#################### Train ####################\n","Epoch 160, Loss: 56.3862 +/- 16.1143, Time: 2.88\n","********** Validate **********\n","Epoch 160, Loss: 55.8611 +/- 15.3949, Time: 1.77\n","\n","#################### Train ####################\n","Epoch 161, Loss: 56.0204 +/- 15.9161, Time: 2.88\n","********** Validate **********\n","Epoch 161, Loss: 55.8245 +/- 16.0468, Time: 1.78\n","\n","#################### Train ####################\n","Epoch 162, Loss: 55.6748 +/- 15.8470, Time: 2.86\n","********** Validate **********\n","Epoch 162, Loss: 56.0457 +/- 15.3276, Time: 1.79\n","\n","#################### Train ####################\n","Epoch 163, Loss: 55.4970 +/- 16.2858, Time: 2.95\n","********** Validate **********\n","Epoch 163, Loss: 55.5153 +/- 15.9162, Time: 1.84\n","\n","#################### Train ####################\n","Epoch 164, Loss: 55.3042 +/- 16.3484, Time: 2.83\n","********** Validate **********\n","Epoch 164, Loss: 56.0414 +/- 16.0864, Time: 1.84\n","\n","#################### Train ####################\n","Epoch 165, Loss: 55.0946 +/- 15.2290, Time: 2.94\n","********** Validate **********\n","Epoch 165, Loss: 54.8548 +/- 16.0387, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 166, Loss: 54.6020 +/- 14.9404, Time: 2.81\n","********** Validate **********\n","Epoch 166, Loss: 54.2369 +/- 14.6058, Time: 1.80\n","\n","#################### Train ####################\n","Epoch 167, Loss: 54.6393 +/- 15.8513, Time: 2.93\n","********** Validate **********\n","Epoch 167, Loss: 53.9954 +/- 15.9807, Time: 1.79\n","\n","#################### Train ####################\n","Epoch 168, Loss: 54.2424 +/- 15.0972, Time: 2.93\n","********** Validate **********\n","Epoch 168, Loss: 53.9586 +/- 15.0982, Time: 1.82\n","\n","#################### Train ####################\n","Epoch 169, Loss: 54.0774 +/- 15.6079, Time: 2.90\n","********** Validate **********\n","Epoch 169, Loss: 53.9916 +/- 15.8369, Time: 1.63\n","\n","#################### Train ####################\n","Epoch 170, Loss: 54.1576 +/- 15.6652, Time: 2.81\n","********** Validate **********\n","Epoch 170, Loss: 53.7808 +/- 15.3171, Time: 1.72\n","\n","#################### Train ####################\n","Epoch 171, Loss: 53.7351 +/- 15.5312, Time: 2.84\n","********** Validate **********\n","Epoch 171, Loss: 53.5028 +/- 15.0995, Time: 1.72\n","\n","#################### Train ####################\n","Epoch 172, Loss: 53.8998 +/- 14.7854, Time: 2.85\n","********** Validate **********\n","Epoch 172, Loss: 53.6078 +/- 15.2758, Time: 1.82\n","\n","#################### Train ####################\n","Epoch 173, Loss: 53.9532 +/- 15.0345, Time: 2.84\n","********** Validate **********\n","Epoch 173, Loss: 53.3567 +/- 14.7905, Time: 1.72\n","\n","#################### Train ####################\n","Epoch 174, Loss: 54.0527 +/- 14.9964, Time: 2.94\n","********** Validate **********\n","Epoch 174, Loss: 53.5101 +/- 15.3251, Time: 1.69\n","\n","#################### Train ####################\n","Epoch 175, Loss: 53.9522 +/- 15.4503, Time: 2.87\n","********** Validate **********\n","Epoch 175, Loss: 53.4138 +/- 15.6130, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 176, Loss: 54.0526 +/- 15.2314, Time: 2.88\n","********** Validate **********\n","Epoch 176, Loss: 53.2074 +/- 15.3304, Time: 1.88\n","\n","#################### Train ####################\n","Epoch 177, Loss: 54.7606 +/- 15.0359, Time: 2.89\n","********** Validate **********\n","Epoch 177, Loss: 53.6877 +/- 15.4148, Time: 1.78\n","\n","#################### Train ####################\n","Epoch 178, Loss: 54.3154 +/- 15.7489, Time: 2.88\n","********** Validate **********\n","Epoch 178, Loss: 54.0153 +/- 15.4085, Time: 1.70\n","\n","#################### Train ####################\n","Epoch 179, Loss: 53.4039 +/- 15.2225, Time: 2.91\n","********** Validate **********\n","Epoch 179, Loss: 53.3330 +/- 15.4054, Time: 1.79\n","\n","#################### Train ####################\n","Epoch 180, Loss: 53.2569 +/- 14.5588, Time: 2.78\n","********** Validate **********\n","Epoch 180, Loss: 52.9502 +/- 14.8479, Time: 1.65\n","\n","#################### Train ####################\n","Epoch 181, Loss: 53.9778 +/- 14.7486, Time: 2.83\n","********** Validate **********\n","Epoch 181, Loss: 52.9513 +/- 14.9980, Time: 1.66\n","\n","#################### Train ####################\n","Epoch 182, Loss: 54.6327 +/- 15.4250, Time: 2.81\n","********** Validate **********\n","Epoch 182, Loss: 55.8368 +/- 14.8623, Time: 1.68\n","\n","#################### Train ####################\n","Epoch 183, Loss: 54.1966 +/- 15.9701, Time: 2.82\n","********** Validate **********\n","Epoch 183, Loss: 53.8032 +/- 15.9885, Time: 1.72\n","\n","#################### Train ####################\n","Epoch 184, Loss: 54.4648 +/- 14.9775, Time: 2.84\n","********** Validate **********\n","Epoch 184, Loss: 54.2853 +/- 15.1576, Time: 1.74\n","\n","#################### Train ####################\n","Epoch 185, Loss: 53.8993 +/- 14.8650, Time: 2.80\n","********** Validate **********\n","Epoch 185, Loss: 54.8315 +/- 16.4692, Time: 1.73\n","\n","#################### Train ####################\n","Epoch 186, Loss: 54.1689 +/- 15.5558, Time: 2.81\n","********** Validate **********\n","Epoch 186, Loss: 52.6763 +/- 15.1883, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 187, Loss: 53.9259 +/- 15.3477, Time: 2.94\n","********** Validate **********\n","Epoch 187, Loss: 54.9552 +/- 16.0494, Time: 1.76\n","\n","#################### Train ####################\n","Epoch 188, Loss: 53.9349 +/- 15.5073, Time: 2.85\n","********** Validate **********\n","Epoch 188, Loss: 53.1649 +/- 14.4634, Time: 1.77\n","\n","#################### Train ####################\n","Epoch 189, Loss: 54.0619 +/- 14.9202, Time: 2.84\n","********** Validate **********\n","Epoch 189, Loss: 55.1070 +/- 15.3566, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 190, Loss: 54.2442 +/- 15.7210, Time: 2.86\n","********** Validate **********\n","Epoch 190, Loss: 52.3212 +/- 14.6391, Time: 1.75\n","\n","#################### Train ####################\n","Epoch 191, Loss: 53.5943 +/- 15.3825, Time: 2.95\n","********** Validate **********\n","Epoch 191, Loss: 53.6493 +/- 15.2739, Time: 1.78\n","\n","#################### Train ####################\n","Epoch 192, Loss: 53.1169 +/- 15.0098, Time: 2.96\n","********** Validate **********\n","Epoch 192, Loss: 52.8364 +/- 14.9562, Time: 1.92\n","\n","#################### Train ####################\n","Epoch 193, Loss: 53.1876 +/- 14.8104, Time: 2.85\n","********** Validate **********\n","Epoch 193, Loss: 52.7349 +/- 15.3048, Time: 1.78\n","\n","#################### Train ####################\n","Epoch 194, Loss: 53.2767 +/- 14.7488, Time: 2.85\n","********** Validate **********\n","Epoch 194, Loss: 53.3406 +/- 15.3074, Time: 1.89\n","\n","#################### Train ####################\n","Epoch 195, Loss: 52.8214 +/- 14.4167, Time: 2.91\n","********** Validate **********\n","Epoch 195, Loss: 52.8003 +/- 14.2916, Time: 1.79\n","\n","#################### Train ####################\n","Epoch 196, Loss: 53.2204 +/- 15.1407, Time: 2.90\n","********** Validate **********\n","Epoch 196, Loss: 52.6500 +/- 15.1435, Time: 1.87\n","\n","#################### Train ####################\n","Epoch 197, Loss: 52.9613 +/- 14.4720, Time: 2.94\n","********** Validate **********\n","Epoch 197, Loss: 52.9921 +/- 14.8035, Time: 1.74\n","\n","#################### Train ####################\n","Epoch 198, Loss: 52.8805 +/- 14.4685, Time: 2.84\n","********** Validate **********\n","Epoch 198, Loss: 52.7375 +/- 14.5164, Time: 1.80\n","\n","#################### Train ####################\n","Epoch 199, Loss: 52.8058 +/- 15.2128, Time: 2.90\n","********** Validate **********\n","Epoch 199, Loss: 53.1959 +/- 13.7948, Time: 1.74\n","\n"]}],"source":["\n","n_iter = 10\n","i = 0\n","for epoch in range(args['epoch_num']):\n","  i += 1\n","  train(train_loader, net, epoch)\n","  validate(test_loader, net, epoch)\n","  if i >= n_iter:\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":677},"executionInfo":{"elapsed":284,"status":"ok","timestamp":1653685198949,"user":{"displayName":"Anderson P","userId":"07518979327659766490"},"user_tz":180},"id":"ZsLkInmYS6tb","outputId":"85d7ce47-7fd3-4253-cc8a-fcc1b20b4c8b"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-03d584e4-b713-4529-91d8-020074ea05be\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ypred</th>\n","      <th>ytest</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>tensor(352.)</td>\n","      <td>tensor(325.0928)</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>tensor(156.)</td>\n","      <td>tensor(87.0831)</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>tensor(12.)</td>\n","      <td>tensor(6.9657)</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>tensor(2.)</td>\n","      <td>tensor(-19.6581)</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>tensor(391.)</td>\n","      <td>tensor(350.4462)</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>tensor(391.)</td>\n","      <td>tensor(335.9232)</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>tensor(84.)</td>\n","      <td>tensor(163.1473)</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>tensor(487.)</td>\n","      <td>tensor(567.8029)</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>tensor(176.)</td>\n","      <td>tensor(326.7634)</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>tensor(157.)</td>\n","      <td>tensor(96.5312)</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>tensor(82.)</td>\n","      <td>tensor(164.3455)</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>tensor(186.)</td>\n","      <td>tensor(211.7070)</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>tensor(277.)</td>\n","      <td>tensor(292.8411)</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>tensor(264.)</td>\n","      <td>tensor(363.8687)</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>tensor(312.)</td>\n","      <td>tensor(318.2643)</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>tensor(56.)</td>\n","      <td>tensor(38.2932)</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>tensor(370.)</td>\n","      <td>tensor(116.5030)</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>tensor(14.)</td>\n","      <td>tensor(31.2116)</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>tensor(124.)</td>\n","      <td>tensor(179.7785)</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>tensor(427.)</td>\n","      <td>tensor(428.0831)</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03d584e4-b713-4529-91d8-020074ea05be')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-03d584e4-b713-4529-91d8-020074ea05be button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-03d584e4-b713-4529-91d8-020074ea05be');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["           ypred             ytest\n","0   tensor(352.)  tensor(325.0928)\n","1   tensor(156.)   tensor(87.0831)\n","2    tensor(12.)    tensor(6.9657)\n","3     tensor(2.)  tensor(-19.6581)\n","4   tensor(391.)  tensor(350.4462)\n","5   tensor(391.)  tensor(335.9232)\n","6    tensor(84.)  tensor(163.1473)\n","7   tensor(487.)  tensor(567.8029)\n","8   tensor(176.)  tensor(326.7634)\n","9   tensor(157.)   tensor(96.5312)\n","10   tensor(82.)  tensor(164.3455)\n","11  tensor(186.)  tensor(211.7070)\n","12  tensor(277.)  tensor(292.8411)\n","13  tensor(264.)  tensor(363.8687)\n","14  tensor(312.)  tensor(318.2643)\n","15   tensor(56.)   tensor(38.2932)\n","16  tensor(370.)  tensor(116.5030)\n","17   tensor(14.)   tensor(31.2116)\n","18  tensor(124.)  tensor(179.7785)\n","19  tensor(427.)  tensor(428.0831)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["Xtest = torch.stack([tup[0] for tup in test_set])\n","Xtest = Xtest.to(args['device'])\n","\n","ytest = torch.stack([tup[1] for tup in test_set])\n","ypred = net(Xtest).cpu().data\n","\n","data = torch.cat((ytest, ypred), axis=1)\n","\n","df_results = pd.DataFrame(data, columns=['ypred', 'ytest'])\n","df_results.head(20)"]}],"metadata":{"accelerator":"GPU","colab":{"name":"Alura_Deeplearning02_Aula04-05_Dataloader_e_Fluxo_de_Treino.ipynb","provenance":[],"authorship_tag":"ABX9TyM5grf90MeSZcAUh0oylLFU"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}